{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28085b9d",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Setting up the Colab environment. DO NOT EDIT!\n",
    "try:\n",
    "  from applied_biostats import setup_environment\n",
    "except ImportError:\n",
    "  !pip -q install applied-biostats-helper\n",
    "  from applied_biostats import setup_environment\n",
    "finally:\n",
    "  grader = setup_environment('Module09_walkthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d2929",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65917c7a",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "At the end of this learning activity you will be able to:\n",
    " - Practice using `pg.normality` and `pg.qqplot` to assess normality.\n",
    " - Practice using `pg.linear_regression` to perform multiple regression.\n",
    " - Interpret the results of linear regression such as the coefficient, p-value, R^2, and confidence intervals.\n",
    " - Describe a _residual_ and how to interpret it.\n",
    " - Relate the _dummy variable trap_ and how to avoid it during regression.\n",
    " - Describe _overfitting_ and how to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade14d1",
   "metadata": {},
   "source": [
    "As we discussed with Dr. Devlin in the introduction video, this week and next we are going to look at HIV neurocognitive impairment data from a cohort here at Drexel.\n",
    "Each person was given a full-scale neuropsychological exam and the resulting values were aggregated and normalized into Z-scores based on demographically matched healthy individuals.\n",
    "\n",
    "In this walkthrough we will explore the effects of antiretroviral medications on neurological impairment.\n",
    "In our cohort, we have two major drug regimens, d4T (Stavudine) and the newer Emtricitabine/tenofovir (Truvada).\n",
    "The older Stavudine is suspected to have neurotoxic effects that are not found in the newer Truvada.\n",
    "We will use inferential statistics to understand this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd77295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pingouin as pg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22605c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hiv_neuro_data.csv')\n",
    "data['education'] = data['education'].astype(float)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3edcaa7",
   "metadata": {},
   "source": [
    "Before we start, we need to talk about assumptions.\n",
    "\n",
    "Basic linear regression has a number assumptions baked into itself:\n",
    " - **Linearity**: The relationship between the independent variables (predictors) and the dependent variable (outcome) is linear. This means that changes in the predictors lead to proportional changes in the dependent variable.\n",
    " - **The relationship between the independent variables and the dependent variable is additive**: The effect of changes in an independent variable X on the dependent variable Y is consistent, regardless of the values of other independent variables. This assumption might not hold if there are interaction effects between independent variables that affect the dependent variable.\n",
    " - **Independence**: Observations are independent of each other. This means that the observations do not influence each other, an assumption that is particularly important in time-series data where time-related dependencies can violate this assumption.\n",
    " - **Homoscedasticity**: The variance of error terms (residuals) is constant across all levels of the independent variables. In other words, as the predictor variable increases, the spread (variance) of the residuals remains constant. This is evaluated at the **end** of the fit.\n",
    " - **Normal Distribution of Errors**: The residuals (errors) of the model are normally distributed. This assumption is especially important for hypothesis testing (e.g., t-tests of coefficients) and confidence interval construction. It's worth noting that for large sample sizes, the Central Limit Theorem helps mitigate the violation of this assumption. This is evaluated at the **end** of the fit.\n",
    " - **Minimal Multicollinearity**: The independent variables need to be independent of each other. Multicollinearity doesn't affect the fit of the model as much as it affects the coefficients' estimates, making them unstable and difficult to interpret.\n",
    " - **No perfect multicollinearity**: Also called the _dummy variable trap_. It states that none of the independent variables should be a perfect linear function of other independent variables. We'll talk more about this when we run into it.\n",
    "\n",
    "Biology itself is highly non-linear.\n",
    "That doesn't mean we can't use linear assumptions to explore biological questions, it just means that we need to be mindful when interpretting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491fae2",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac30e8",
   "metadata": {},
   "source": [
    "Let's start by plotting the each variable against EDZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (age_ax, edu_ax, ys_ax) = plt.subplots(1,3, sharey=True, figsize = (15, 5))\n",
    "\n",
    "sns.regplot(data = data,\n",
    "            x = 'age',\n",
    "            y = 'exec_domain_z',\n",
    "            ax=age_ax)\n",
    "\n",
    "sns.regplot(data = data,\n",
    "            x = 'education',\n",
    "            y = 'exec_domain_z',\n",
    "            ax=edu_ax)\n",
    "\n",
    "sns.regplot(data = data,\n",
    "            x = 'YearsSeropositive',\n",
    "            y = 'exec_domain_z',\n",
    "            ax=ys_ax)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1194d2",
   "metadata": {},
   "source": [
    "### Q1: By inspection, which variable is most correlated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef6bdd",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "|               |    |\n",
    "| --------------|----|\n",
    "| Points        | 5  |\n",
    "| Public Checks | 3  |\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f421998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer: age, education, YearsSeropositive\n",
    "q1_most_correlated = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46655339",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_initial_correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693675c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (race_ax, sex_ax, art_ax) = plt.subplots(1,3, sharey=True, figsize = (15, 5))\n",
    "\n",
    "sns.stripplot(data=data,\n",
    "            x = 'race',\n",
    "            y = 'exec_domain_z', ax=race_ax)\n",
    "sns.boxplot(data=data,\n",
    "            x = 'race',\n",
    "            y = 'exec_domain_z', ax=race_ax)\n",
    "\n",
    "sns.stripplot(data=data,\n",
    "            x = 'sex',\n",
    "            y = 'exec_domain_z', ax=sex_ax)\n",
    "sns.boxplot(data=data,\n",
    "            x = 'sex',\n",
    "            y = 'exec_domain_z', ax=sex_ax)\n",
    "\n",
    "sns.stripplot(data=data,\n",
    "            x = 'ART',\n",
    "            y = 'exec_domain_z', ax=art_ax)\n",
    "sns.boxplot(data=data,\n",
    "            x = 'ART',\n",
    "            y = 'exec_domain_z', ax=art_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666128e",
   "metadata": {},
   "source": [
    "### Q2: By inspection, which variable has the most between class difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58322e",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "|               |    |\n",
    "| --------------|----|\n",
    "| Points        | 5  |\n",
    "| Public Checks | 3  |\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cd11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer: race, sex, ART\n",
    "q2_most_bcd = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576b2fd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_initial_bcd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb924d84",
   "metadata": {},
   "source": [
    "## Basic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7581c8",
   "metadata": {},
   "source": [
    "We'll start by taking the simplest approach and regress the most correlated value first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e3418",
   "metadata": {},
   "source": [
    "`pg.linear_regression` works by regressing all columns in the first parameter against the single column in the second.\n",
    "By convention, we usually use the variables `X` and `y`.\n",
    "\n",
    "You'll often see this written as:\n",
    "\n",
    "$\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "In the case of `pg.linear_regression` the $\\boldsymbol{\\epsilon}$ is added by default and we do not need to specify it.\n",
    "\n",
    "You do not have to use the variable names `X` and `y`, in many cases you might have multiple `X`s and `y`s, but for simplicity, I will stick with this simple convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649942cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['YearsSeropositive'] # Our independent variables\n",
    "y = data['exec_domain_z']     # Our dependent variable\n",
    "res = pg.linear_regression(X, y)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6908128",
   "metadata": {},
   "source": [
    "This has fit the equation:\n",
    "\n",
    "`PDZ = -0.035*YS + 0.712`\n",
    "\n",
    "It tells us that the likelihood of this slope being zero is 1.0E-20 and that years-seropositive explains ~23.6% of variation in EDZ that we observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406fd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(data = data,\n",
    "                 x = 'YearsSeropositive',\n",
    "                 y = 'exec_domain_z')\n",
    "\n",
    "# Pick \"years seropositive\" from 0 to 70\n",
    "x = np.arange(0, 70)\n",
    "\n",
    "# Use the coefficients from above in a linear equation\n",
    "y = res.loc[1, 'coef']*x + res.loc[0, 'coef']\n",
    "\n",
    "ax.plot(x, y, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea91c10",
   "metadata": {},
   "source": [
    "## Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a6cfa",
   "metadata": {},
   "source": [
    "_Residuals_ are the difference between the observed value and the predicted value.\n",
    "In the case of a simple linear regression, this is the y-distance between each point and the best-fit line.\n",
    "Examining these is an import step in assessing the fit for any biases.\n",
    "You can think of the residual as what is \"left over\" after the regression.\n",
    "\n",
    "We could calculate these ourselves from the regression coefficients, but, `pingouin` conviently provides them for us.\n",
    "The result `DataFrame` from `pg.linear_regression` has a special attribute `.residuals_` which stores the difference between the prediction and reality for each point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.residuals_[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a4e33",
   "metadata": {},
   "source": [
    "In order to test the **Homoscedasticity** we want to ensure that these residuals are _not correlated with the depenendant variable_.\n",
    "\n",
    "In our case, this means that the model is equally good predicting the EDZ of people recently infected with HIV and those who have been living with HIV for a long time.\n",
    "\n",
    "To do this, we plot the residuals vs each independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=data['YearsSeropositive'],  y=res.residuals_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda72572",
   "metadata": {},
   "source": [
    "This is an ideal residual plot.\n",
    "It should look like a random \"stary-night sky\" centered around 0.\n",
    "This implies that the model is not better or worse for any given X value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e88147",
   "metadata": {},
   "source": [
    "Let's also test our assumption about a normal distribution of errors of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95daa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q3: Are the residuals normally distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86f330",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "|               |    |\n",
    "| --------------|----|\n",
    "| Points        | 5  |\n",
    "| Public Checks | 5  |\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113544a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Q-Q plot of the residuals\n",
    "\n",
    "q3_plot = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b0b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the Jarque-Bera normal test for large sample sizes\n",
    "\n",
    "q3_norm_res = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbed90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Are the residuals normally distributed? 'yes' or 'no'\n",
    "\n",
    "q3_is_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042db170",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_resid_normality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44835eac",
   "metadata": {},
   "source": [
    "You don't need to do this test at every stage, but it is a good test to do before you are _done_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985a9ae",
   "metadata": {},
   "source": [
    "## Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5dd7e",
   "metadata": {},
   "source": [
    "Regression is not limited to a single independent variable, you can add as many as you'd like.\n",
    "\n",
    "In our case, there are two others that we should consider: `age` and `education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed82b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['YearsSeropositive', 'education', 'age']]\n",
    "y = data['exec_domain_z']\n",
    "res = pg.linear_regression(X, y)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75edbd",
   "metadata": {},
   "source": [
    "Now, it has fit the equation:\n",
    "\n",
    "`EDZ = -0.037*YS - 0.103*edu + 0.019*age + 0.977`\n",
    "\n",
    "The education is significant at p=8.17E-7.\n",
    "Be caution when comparing coefficients, we might be tempted to compare -0.0422 and -0.0506 and say that education has a more negative effect than YS ...\n",
    "But, remember that education ranges from 0-12 and YS ranges from 0-60, these are not on the same scale and are not directly comparable.\n",
    "We'll talk about how to compare relative importance later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3749aa2",
   "metadata": {},
   "source": [
    "As before, we should check the residuals of the model against _each_ independent variable in the regression to check for homoscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ys_ax, edu_ax, age_ax) = plt.subplots(1,3, sharey=True, figsize = (15, 5))\n",
    "\n",
    "sns.scatterplot(x=data['YearsSeropositive'],  y=res.residuals_, ax=ys_ax)\n",
    "sns.scatterplot(x=data['education'],  y=res.residuals_, ax=edu_ax)\n",
    "sns.scatterplot(x=data['age'],  y=res.residuals_, ax=age_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff343ab",
   "metadata": {},
   "source": [
    "Three more stary night skies. Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86388474",
   "metadata": {},
   "source": [
    "Remember, the residual is the difference between the prediction of the model and reality.\n",
    "Therefore, we can also use the residual plots to see how well the regression is handling other variables we have not included in the model.\n",
    "If the model has properly accounted for something, the residual plot should stay centered around 0.\n",
    "\n",
    "This can be done for categorical or continious variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (race_ax, sex_ax, art_ax) = plt.subplots(1,3, sharey=True, figsize = (15, 5))\n",
    "\n",
    "race_ax.set_ylabel('residual')\n",
    "\n",
    "sns.barplot(x=data['race'],  y=res.residuals_, ax=race_ax)\n",
    "sns.barplot(x=data['sex'],  y=res.residuals_, ax=sex_ax)\n",
    "sns.barplot(x=data['ART'],  y=res.residuals_, ax=art_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4690c5",
   "metadata": {},
   "source": [
    "Here we see some interesting patterns:\n",
    " - The graph of race against residuals shows us that our model is signifacntly racially biased. AA individuals are significantly 'under-estimated' by the model, C individauals are significantly over-estimated, and H individuals are significantly over-estimated.\n",
    " - The graph of sex shows that there is no real difference in the residuals. It has accounted for sex already.\n",
    " - It looks like there is a real difference across ART."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd041550",
   "metadata": {},
   "source": [
    "## _ANCOVA_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca71e5c",
   "metadata": {},
   "source": [
    "What we have done above is create a model that _accounts_ for the effects of age, education, and YS on EDZ.\n",
    "We **subtracted** that effect (the predicted value) from the observed value thus creating the _residual_.\n",
    "This is what is \"left over\" in the observed value after accounting for covariates or nuisance variables.\n",
    "Then we plotted the _residual_ against each of our categorical variables.\n",
    "If we then took the ANOVA of these residuals we'd be testing the hypothesis:\n",
    " _When accounting for age, education, and YS is there a difference across race._\n",
    " \n",
    "This process is called an _Analysis of covariance_ or an **ANCOVA**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9e4a8",
   "metadata": {},
   "source": [
    "### Standard first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf633a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q4: Perform an ANOVA between ART on the Executive Domain Z-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2076ac6",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "|               |    |\n",
    "| --------------|----|\n",
    "| Points        | 5  |\n",
    "| Public Checks | 4  |\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e8854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot showing the effect of ART on EDZ\n",
    "q4_plot = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4d1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform an ANOVA testing the impact of ART on EDZ\n",
    "q4_res = ...\n",
    "q4_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc9c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does ART have a significant impact on Executive Domain? 'yes' or 'no'?\n",
    "\n",
    "q4_art_impact = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f504f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_art_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de061a67",
   "metadata": {},
   "source": [
    "### With correction\n",
    "\n",
    "Nicely `pingouin` has something built right in to do this whole process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=data['ART'],  y=res.residuals_)\n",
    "\n",
    "# An ANCOVA testing the impact of ART on EDZ\n",
    "# after correcting for the impace of age, education and YS\n",
    "pg.ancova(data,\n",
    "          dv = 'exec_domain_z',\n",
    "          between = 'ART',\n",
    "          covar=['YearsSeropositive', 'education', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c3b54",
   "metadata": {},
   "source": [
    "We can notice that after correction for covaraites the F-value has increased and the p-value has decreased.\n",
    "This means the analysis is attributing more difference to race after correction and is more sure this is not due to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2f67e",
   "metadata": {},
   "source": [
    "The _advantage_ of using the `pg.ancova` function is that you can easily and quickly do your analysis.\n",
    "The _disadvantage_ is that you cannot examine the internal regression for Normality and Homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec1945",
   "metadata": {},
   "source": [
    "But, what if we wanted to have a covariate that is a category like race?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb7d9",
   "metadata": {},
   "source": [
    "## Regression with categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2634226",
   "metadata": {},
   "source": [
    "So, how do you do regression with a category like race?\n",
    "\n",
    "Could it be as simple as adding it the `X` matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data[['YearsSeropositive', 'education', 'age', 'race']]\n",
    "# y = data['processing_domain_z']\n",
    "# res = pg.linear_regression(X, y)\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffe95d",
   "metadata": {},
   "source": [
    "Would have been nice, but we need to get a little tricky and use _dummy_ variables.\n",
    "\n",
    "In their simplest terms, dummy variables are binary representations of categories.\n",
    "Like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['race']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c736ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the same continious variables\n",
    "X = data[['YearsSeropositive', 'education', 'age']]\n",
    "\n",
    "# Creating new dummy variables for race\n",
    "dummy_vals = pd.get_dummies(data['race']).astype(float)\n",
    "\n",
    "\n",
    "# Adding them the end\n",
    "X = pd.concat([X, dummy_vals], axis=1)\n",
    "\n",
    "y = data['exec_domain_z']\n",
    "\n",
    "res = pg.linear_regression(X, y)\n",
    "res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9e308",
   "metadata": {},
   "source": [
    "This _Warning_ is telling us that our model has fallen into the _dummy variable trap_.\n",
    "The dummy variable trap occurs when dummy variables created for categorical data in a regression model are perfectly collinear, meaning one variable can be predicted from the others, leading to redundancy.\n",
    "This happens because the inclusion of all dummy variables for a category along with a constant term (intercept) creates a situation where the sum of the dummy variables plus the intercept equals one, introducing perfect multicollinearity.\n",
    "To avoid this, one dummy variable should be dropped to serve as the reference category, ensuring the model's design matrix is full rank and the regression coefficients are estimable and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data['race'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eece70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['YearsSeropositive', 'education', 'age']]\n",
    "dummy_vals = pd.get_dummies(data['race'], drop_first=True).astype(float)\n",
    "X = pd.concat([X, dummy_vals], axis=1)\n",
    "y = data['exec_domain_z']\n",
    "res = pg.linear_regression(X, y)\n",
    "res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec05ba",
   "metadata": {},
   "source": [
    "We can notice a few things here:\n",
    " - **AA** has become the 'reference', the coefficients of C and H are relative to AA, which is set at 0.\n",
    "   - C individuals have a decreased score (relative to AA), which is significant.\n",
    "   - H individuals have an decreased score (relative to AA), which is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0afe8",
   "metadata": {},
   "source": [
    "We can look at the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (race_ax, sex_ax, art_ax) = plt.subplots(1,3, sharey=True, figsize = (15, 5))\n",
    "\n",
    "race_ax.set_ylabel('residual')\n",
    "\n",
    "sns.barplot(x=data['race'],  y=res.residuals_, ax=race_ax)\n",
    "sns.barplot(x=data['sex'],  y=res.residuals_, ax=sex_ax)\n",
    "sns.barplot(x=data['ART'],  y=res.residuals_, ax=art_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d6361",
   "metadata": {},
   "source": [
    "Let's merge everything into a single analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d309f6b",
   "metadata": {},
   "source": [
    "### Final combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([data[['YearsSeropositive', 'education', 'age']],\n",
    "               pd.get_dummies(data['race'], drop_first=True).astype(float),\n",
    "               pd.get_dummies(data['sex'], drop_first=True).astype(float),\n",
    "               pd.get_dummies(data['ART'], drop_first=True).astype(float),\n",
    "              ], axis=1)\n",
    "y = data['exec_domain_z']\n",
    "res = pg.linear_regression(X, y)\n",
    "res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84888e",
   "metadata": {},
   "source": [
    "Here our _reference_ is an AA, female taking Stavudine.\n",
    " - Everything is signifiant except for sex.\n",
    " - We see that Truvada has a _significant positive_ effect on EDZ relative to Stavudine.\n",
    "\n",
    "Since this is our final model, let's test our last normality assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.qqplot(res.residuals_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f246c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.normality(res.residuals_, method='normaltest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c570a3",
   "metadata": {},
   "source": [
    "Perfect, now we know that our final model passes the _Normal Distribution of Errors_ assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7628161",
   "metadata": {},
   "source": [
    "What about understanding which parameters have the largest impact on the model?\n",
    "Stated another way: which features are most important to determing EDZ?\n",
    "\n",
    "Nicely, `pingouin` can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f277fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_with_imp = pg.linear_regression(X, y, relimp=True)\n",
    "res_with_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c24a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After filtering and sorting\n",
    "res_with_imp.query('pval<0.01').sort_values('relimp_perc', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca122b",
   "metadata": {},
   "source": [
    "## Over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e338cd",
   "metadata": {},
   "source": [
    "In principle we can continue to add more and more variables to the `X` and just let the computer figure out the p-value of each.\n",
    "\n",
    "There are a few reasons we shouldn't take this tack.\n",
    " - **Overfitting** : A larger model will **ALWAYS** fit better than a smaller model. This doesn't mean the larger model is **better** at predicting _all samples_, it just means it fits **these** samples better.\n",
    " - **Explainability** : Large models with many parameters are difficult to explain and reason about. We are biologists, not data scientists. Our job is to reason about the _result_ of the analysis, not create the best fitting model.\n",
    " - **Statistical power** : As you add more noise features you lose the power to detect real features.\n",
    "\n",
    "So, you should limit yourself to only those features that you think are biologically meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47844c51",
   "metadata": {},
   "source": [
    "When planning experiments there are a couple of things you can do to avoid overfitting:\n",
    " - **Sample size** : While there is no strict rule, you should plan to have _at least_ 10 samples per feature in your model.\n",
    " - **Even sampling** : It is ideal to have a roughly equal representation of the entire parameter space. If you have categories, you should have an equal number of each. If you have continious data, you should have both high and low values. If you have many parameters, you should have an equal number of each of their interactions as well.\n",
    "\n",
    "These are good guidelines for all model-fitting style analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec752e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features:', len(X.columns))\n",
    "print('Obs:', len(X.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92a5a7",
   "metadata": {},
   "source": [
    "## Even more regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03a91f",
   "metadata": {},
   "source": [
    "There are a number of regression based tools in `pingouin` that we didn't cover that may be useful to explore.\n",
    " - `pg.logistic_regression` : This works similar to linear regression but is for binary dependent variables.\n",
    "Each feature is regressed to create an equation that estimates the likelihood of the `dv` being `True`.\n",
    " - `pg.partial_corr` : Like the ANCOVA, this is a tool for removing the effect of covariates and then calculating a correlation coefficient.\n",
    " - `pg.rm_corr` : Correlation with repeated measures. This is useful if you have measured the same _sample_ multiple times and want to account for intermeasurment variability.\n",
    " - `pg.mediation_analysis` : Tests the hypothesis that the independent variable `X` influences the dependent variable `Y` by a change in mediator `M`; like so `X -> M -> Y`.\n",
    "This is useful to disentangle causal effects from covariation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e46100",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512dc02",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "## Submission\n",
    "\n",
    "You do not need to submit this walkthrough notebook.\n",
    "Simply complete the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "otter": {
   "assignment_name": "Module09_walkthrough"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
